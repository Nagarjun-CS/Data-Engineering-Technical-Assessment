{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8051d7b8-00f6-429a-b937-677c0716eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, initcap, regexp_replace, when, to_date, lit, expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40370d0f-9e99-4533-a285-7a657443ce1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_config_from_json(config_path):\n",
    "    config_df = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    config_row = config_df.first()\n",
    "\n",
    "    partner_code = config_row[\"partner_code\"]\n",
    "    delimiter = config_row[\"delimiter\"]\n",
    "    date_format = config_row[\"date_format\"]\n",
    "    column_mapping = config_row[\"column_mapping\"].asDict() \n",
    "\n",
    "    return partner_code, delimiter, column_mapping, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d05e6-0d35-4823-b470-a84d2b4c5e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize_columns(df, dob_format):\n",
    "    df = df.withColumn(\"first_name\", initcap(col(\"first_name\"))) \\\n",
    "           .withColumn(\"last_name\", initcap(col(\"last_name\"))) \\\n",
    "           .withColumn(\"email\", lower(col(\"email\"))) \\\n",
    "           .withColumn(\"dob\", expr(f\"try_to_date(dob, '{dob_format}')\")) \\\n",
    "           .withColumn(\"phone\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\")) \\\n",
    "           .withColumn(\"phone\", when(col(\"phone\").rlike(r\"^\\d{10}$\"),\n",
    "                                     regexp_replace(col(\"phone\"), r\"(\\d{3})(\\d{3})(\\d{4})\", r\"$1-$2-$3\"))\n",
    "                       .otherwise(col(\"phone\")))\n",
    "    df = df.withColumn(\"is_valid\", col(\"external_id\").isNotNull())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4538642b-affb-4e71-93a2-7ce3b7dbe481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_and_process(data_path, config_path):\n",
    "    partner_code, delimiter, column_mapping, dob_format  = load_config_from_json(config_path)\n",
    "\n",
    "    # Read the raw file\n",
    "    raw_df = spark.read.option(\"header\", True).option(\"delimiter\", delimiter).csv(data_path)\n",
    "\n",
    "    # TRIM column names (remove extra spaces)\n",
    "    for col_name in raw_df.columns:\n",
    "        raw_df = raw_df.withColumnRenamed(col_name, col_name.strip())\n",
    "\n",
    "    # Rename columns to standardized schema\n",
    "    df = raw_df.select([col(src).alias(dst) for src, dst in column_mapping.items()])\n",
    "\n",
    "    # Clean and tag\n",
    "    df = standardize_columns(df, dob_format)  \n",
    "    return df.withColumn(\"partner_code\", lit(partner_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9612a611-d27f-4e07-bc70-f2b2265d0707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# (Change this path according the requirement. I have used the volumes to store the data and the config json of the partners, locally on databricks)\n",
    "\n",
    "config_dir = \"/Volumes/data_catalog/source/assessment_volume/Config\"\n",
    "data_dir = \"/Volumes/data_catalog/source/assessment_volume/Healthcare_partners\"\n",
    "\n",
    "partner_dfs = []\n",
    "\n",
    "# Loop through all config files\n",
    "for config_path in Path(config_dir).glob(\"*.json\"):\n",
    "    config_path_str = str(config_path)\n",
    "    partner_code, _, _, _ = load_config_from_json(config_path_str)\n",
    "\n",
    "    # Find any file in data_dir starting with the partner_code (any extension)\n",
    "    matching_files = list(Path(data_dir).glob(f\"{partner_code}.*\"))\n",
    "    if not matching_files:\n",
    "        print(f\"No data file found for partner '{partner_code}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Take the first matching file\n",
    "    data_file_path = str(matching_files[0])\n",
    "\n",
    "    # Load and process\n",
    "    df = load_and_process(data_file_path, config_path_str)\n",
    "    partner_dfs.append(df)\n",
    "\n",
    "# Merge all dataframes\n",
    "if partner_dfs:\n",
    "    final_df = partner_dfs[0]\n",
    "    for df in partner_dfs[1:]:\n",
    "        final_df = final_df.unionByName(df)\n",
    "\n",
    "    # Filter out rows with null DOBs\n",
    "    merged_df = final_df.filter(col(\"dob\").isNotNull())\n",
    "    merged_df.display()\n",
    "else:\n",
    "    print(\"No partner data was loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b9e673-bd5e-4012-939d-d2b9b5e31c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows with null DOB\n",
    "invalid_dob_df = final_df.filter(col(\"dob\").isNull())\\\n",
    "                            .drop(\"is_valid\")\n",
    "\n",
    "# (change this path according the requirement, where you want to store the malformed rows in order to send it to the partners for clarification. I have used volumes to store the files locally)\n",
    "\n",
    "output_path = \"/Volumes/data_catalog/source/assessment_volume/output/invalid_jobs\"\n",
    "\n",
    "if invalid_dob_df.limit(1).count() == 0:\n",
    "    # Delete entire output folder if no invalid DOBs\n",
    "    dbutils.fs.rm(output_path, True)\n",
    "    print(\"No invalid DOBs found. Old data cleaned.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    invalid_dob_df.display()\n",
    "\n",
    "    partner_codes_df = invalid_dob_df.select(\"partner_code\").distinct()\n",
    "\n",
    "    partner_codes = [row[\"partner_code\"] for row in partner_codes_df.collect()]\n",
    "\n",
    "    # Loop and write each partnerâ€™s invalid DOB rows separately\n",
    "    for partner in partner_codes:\n",
    "        partner_df = invalid_dob_df.filter(col(\"partner_code\") == partner)\n",
    "        partner_df = partner_df.drop(\"partner_code\")\n",
    "        partner_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/{partner}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
