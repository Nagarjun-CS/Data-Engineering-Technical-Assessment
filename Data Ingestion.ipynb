{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8051d7b8-00f6-429a-b937-677c0716eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, initcap, regexp_replace, when, to_date, lit, expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40370d0f-9e99-4533-a285-7a657443ce1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_config_from_json(config_path):\n",
    "    config_df = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    config_row = config_df.first()\n",
    "\n",
    "    partner_code = config_row[\"partner_code\"]\n",
    "    delimiter = config_row[\"delimiter\"]\n",
    "    date_format = config_row[\"date_format\"]\n",
    "    column_mapping = config_row[\"column_mapping\"].asDict() \n",
    "\n",
    "    return partner_code, delimiter, column_mapping, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d05e6-0d35-4823-b470-a84d2b4c5e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize_columns(df, dob_format):\n",
    "    df = df.withColumn(\"first_name\", initcap(col(\"first_name\"))) \\\n",
    "           .withColumn(\"last_name\", initcap(col(\"last_name\"))) \\\n",
    "           .withColumn(\"email\", lower(col(\"email\"))) \\\n",
    "           .withColumn(\"dob\", expr(f\"try_to_date(dob, '{dob_format}')\")) \\\n",
    "           .withColumn(\"phone\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\")) \\\n",
    "           .withColumn(\"phone\", when(col(\"phone\").rlike(r\"^\\d{10}$\"),\n",
    "                                     regexp_replace(col(\"phone\"), r\"(\\d{3})(\\d{3})(\\d{4})\", r\"$1-$2-$3\"))\n",
    "                       .otherwise(col(\"phone\")))\n",
    "    df = df.withColumn(\"is_valid\", col(\"external_id\").isNotNull())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4538642b-affb-4e71-93a2-7ce3b7dbe481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_and_process(data_path, config_path):\n",
    "    partner_code, delimiter, column_mapping, dob_format  = load_config_from_json(config_path)\n",
    "\n",
    "    # Read the raw file\n",
    "    raw_df = spark.read.option(\"header\", True).option(\"delimiter\", delimiter).csv(data_path)\n",
    "\n",
    "    # TRIM column names (remove extra spaces)\n",
    "    for col_name in raw_df.columns:\n",
    "        raw_df = raw_df.withColumnRenamed(col_name, col_name.strip())\n",
    "\n",
    "    # Rename columns to standardized schema\n",
    "    df = raw_df.select([col(src).alias(dst) for src, dst in column_mapping.items()])\n",
    "\n",
    "    # Clean and tag\n",
    "    df = standardize_columns(df, dob_format)  \n",
    "    return df.withColumn(\"partner_code\", lit(partner_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dd17ba-228e-4dbb-8ec7-ea166f18cf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "acme_data_path = \"/Volumes/data_catalog/source/assessment_volume/Healthcare_patners/acme.txt\"\n",
    "bettercare_data_path = \"/Volumes/data_catalog/source/assessment_volume/Healthcare_patners/bettercare.csv\"\n",
    "\n",
    "# Config paths \n",
    "acme_config_path = \"/Volumes/data_catalog/source/assessment_volume/Config/acme.json\"\n",
    "bettercare_config_path = \"/Volumes/data_catalog/source/assessment_volume/Config/bettercare.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9612a611-d27f-4e07-bc70-f2b2265d0707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest both datasets\n",
    "acme_df = load_and_process(acme_data_path, acme_config_path)\n",
    "bettercare_df = load_and_process(bettercare_data_path, bettercare_config_path)\n",
    "\n",
    "# Merge into unified output\n",
    "final_df = acme_df.unionByName(bettercare_df)\n",
    "\n",
    "merged_df = final_df.filter(col(\"dob\").isNotNull())\n",
    "merged_df.display() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b9e673-bd5e-4012-939d-d2b9b5e31c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows with null DOB\n",
    "invalid_dob_df = final_df.filter(col(\"dob\").isNull())\\\n",
    "                            .drop(\"is_valid\")\n",
    "\n",
    "output_path = \"/Volumes/data_catalog/source/assessment_volume/output/invalid_dobs\"\n",
    "\n",
    "if invalid_dob_df.limit(1).count() == 0:\n",
    "    # Delete entire output folder if no invalid DOBs\n",
    "    dbutils.fs.rm(output_path, True)\n",
    "    print(\"No invalid DOBs found. Old data cleaned.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    invalid_dob_df.display()\n",
    "\n",
    "    # Get all partner codes (as list of Row objects)\n",
    "    partner_codes_df = invalid_dob_df.select(\"partner_code\").distinct()\n",
    "\n",
    "    # Use collect() on DataFrame (NOT RDD) — supported in serverless\n",
    "    partner_codes = [row[\"partner_code\"] for row in partner_codes_df.collect()]\n",
    "\n",
    "    # Loop and write each partner’s invalid DOB rows separately\n",
    "    for partner in partner_codes:\n",
    "        partner_df = invalid_dob_df.filter(col(\"partner_code\") == partner)\n",
    "        partner_df = partner_df.drop(\"partner_code\")\n",
    "        partner_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/{partner}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
